{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8da195-5132-4579-894a-dd9a674eebda",
   "metadata": {},
   "source": [
    "## Reworking the file pathing procedure for CMIP6 models:\n",
    "---\n",
    "\n",
    "#### Objectives:\n",
    "- Needed to rewrite the file path retrevial for historical and SSP245 to have a baseline to compare results to\n",
    "- Needed to make the 10_12_24 ipynb file clearer - it was written in the pre christmas delusional coding spree\n",
    "- learned that theres a dependance on grid choice - so need to filter those when coming up with the reduced version\n",
    "- Need to the latwise calculations and alternative form of the theory so I might as well start from a good place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4f1f95a-f7a7-4d94-853e-e2783b4b72f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/users/chingosa/TropExt/Functions/')\n",
    "import CMIPFuncs as func\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyesgf.search import SearchConnection\n",
    "os.environ[\"ESGF_PYCLIENT_NO_FACETS_STAR_WARNING\"] = \"on\"\n",
    "\n",
    "def extract_varientID(string):  # also Known as extract_8_chars\n",
    "    start_index = string.find('_r') \n",
    "    end_index = string.find('_', start_index+1)\n",
    "    return string[start_index+1:end_index]\n",
    "\n",
    "def extract_monType(string):  # also Known as extract_8_chars\n",
    "    start_index = string.find('_') \n",
    "    end_index = string.find('_', start_index+1)\n",
    "    return string[start_index+1:end_index]\n",
    "\n",
    "def extract_SimNums(string):\n",
    "    try:\n",
    "        start_index = string.find('_r')\n",
    "        end_index = string.find('i', start_index)\n",
    "        r = int(string[start_index + 2:end_index])\n",
    "\n",
    "        start_index = string.find('i', end_index)\n",
    "        end_index = string.find('p', start_index)\n",
    "        i = int(string[start_index + 1:end_index])\n",
    "\n",
    "        start_index = string.find('p', end_index)\n",
    "        end_index = string.find('f', start_index)\n",
    "        p = int(string[start_index + 1:end_index])\n",
    "\n",
    "        start_index = string.find('f', end_index)\n",
    "        end_index = string.find('_', start_index)\n",
    "        f = int(string[start_index + 1:end_index])\n",
    "        \n",
    "        return r, i, p, f\n",
    "    except Exception as e:\n",
    "        # Provide feedback for debugging\n",
    "        print(f\"Error processing string '{string}': {e}\")\n",
    "        return None, None, None, None\n",
    "   \n",
    "def getPeriod(filename):\n",
    "    if 'abrupt-4xCO2' in filename: return 'abrupt-4xCO2'\n",
    "    elif 'piControl' in filename: return 'piControl'\n",
    "    elif 'historical' in filename: return 'historical'\n",
    "    elif 'ssp245' in filename: return 'ssp245'\n",
    "    \n",
    "def whichVar(filename):\n",
    "    if 'tas' in filename: return 'tas'\n",
    "    elif 'huss' in filename: return 'huss'\n",
    "    elif 'hurs' in filename: return 'hurs'\n",
    "    elif 'ps' in filename: return 'ps'\n",
    "\n",
    "def whichTimestep(filename):\n",
    "    if 'day' in filename: return 'day'\n",
    "    elif 'mon' in filename: return 'mon'\n",
    "    else : return np.nan\n",
    "\n",
    "def whichGrid(filename):\n",
    "    varID = extract_varientID(filename)\n",
    "    start_index = filename.find(varID) +len(varID)\n",
    "    end_index = filename.find('_', start_index+1)\n",
    "    return filename[start_index+1:end_index]\n",
    "\n",
    "def extractDates(string):\n",
    "    ncFind = string.find('.nc') \n",
    "    ncBack = string.rfind('-', 0, ncFind)\n",
    "    dback = (ncFind-ncBack)-1\n",
    "    stop = string[ncBack+1:ncFind]\n",
    "    start = string[ncBack-dback:ncBack]\n",
    "    try:\n",
    "        return extractYear(start), extractYear(stop)\n",
    "    except:\n",
    "        print(string)\n",
    "def extractYear(Date):\n",
    "    return int(Date[:4])\n",
    "def getExperiment(filename):\n",
    "    if 'abrupt-4xCO2' in filename: return '4x'\n",
    "    elif 'piControl' in filename: return '4x'\n",
    "    elif 'historical' in filename: return 'SSP245'\n",
    "    elif 'ssp245' in filename: return 'SSP245'\n",
    "\n",
    "# conn = SearchConnection('https://esgf.ceda.ac.uk/esg-search', distrib=True) #UK one\n",
    "conn = SearchConnection('https://esgf-node.llnl.gov/esg-search', distrib=True) #German one\n",
    "# conn = SearchConnection('https://esgf-data.dkrz.de/esg-search', distrib=True)  #US one\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## This comes up with a list of agencies and models - the models list is what we will use to search\n",
    "Agencies = os.listdir(f'/badc/cmip6/data/CMIP6/CMIP/')\n",
    "Models = [os.listdir(f'/badc/cmip6/data/CMIP6/CMIP/{Agency}') for Agency in Agencies]\n",
    "models = []\n",
    "for sublist in Models:\n",
    "    models.extend(sublist)\n",
    "\n",
    "print(len(models))\n",
    "\n",
    "def findFilePaths(model): \n",
    "    if os.path.isfile(f'/home/users/chingosa/TropExt/CMIP6_Analysis/TempData/Paths_{model}.csv'):\n",
    "        print('skip')\n",
    "        return None\n",
    "    else:\n",
    "        print(model) # for monitoring progress of parallel execution\n",
    "        files = []\n",
    "        for exid in ['abrupt-4xCO2','piControl','historical','ssp245']:\n",
    "            for var in ['tas', 'huss', 'ps']:\n",
    "                query = conn.new_context(project=\"CMIP6\",     \n",
    "                                         experiment_id=exid,\n",
    "                                         source_id = model,\n",
    "                                         frequency = 'day', \n",
    "                                         # member_id=\"r1i1p1f1\",\n",
    "                                         variable_id = var)\n",
    "\n",
    "       \n",
    "                results = query.search()\n",
    "                for i in range(len(results)):\n",
    "                    try:\n",
    "                        hit = results[i].file_context().search()\n",
    "                    except:\n",
    "                        hit = results[i].file_context().search()\n",
    "                    files += list(map(lambda f: {'model': model,\n",
    "                                                 'filename': f.filename, \n",
    "                                                 'download_url': f.download_url, \n",
    "                                                 'opendap_url': f.opendap_url}, hit))\n",
    "                \n",
    "                if (len(results) == 0) & (var == 'ps'):\n",
    "                    query = conn.new_context(project=\"CMIP6\",     \n",
    "                                         experiment_id=exid,\n",
    "                                         source_id = model,\n",
    "                                         frequency = 'mon', \n",
    "                                         # member_id=\"r1i1p1f1\",\n",
    "                                         variable_id = var)\n",
    "\n",
    "       \n",
    "                    results = query.search()\n",
    "                    for i in range(len(results)):\n",
    "                        try:\n",
    "                            hit = results[i].file_context().search()\n",
    "                        except:\n",
    "                            hit = results[i].file_context().search()\n",
    "                        files += list(map(lambda f: {'model': model,\n",
    "                                                 'filename': f.filename, \n",
    "                                                 'download_url': f.download_url, \n",
    "                                                 'opendap_url': f.opendap_url}, hit))\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(files)\n",
    "        if len(df) == 0: return None\n",
    "        df = df.dropna() # some opendap_urls are not found - so get rid of those\n",
    "        df[['r', 'i', 'p', 'f']] = pd.DataFrame(\n",
    "            df['filename'].apply(extract_SimNums).tolist(),\n",
    "            index=df.index\n",
    "        )\n",
    "                \n",
    "        df['Varient'] = df.filename.apply(extract_varientID)\n",
    "        df['period'] = df.filename.apply(getPeriod)\n",
    "        df['Var'] = df.filename.apply(whichVar)\n",
    "        df['grid'] = df.filename.apply(whichGrid)\n",
    "        df['timeStep'] = df.filename.apply(whichTimestep)\n",
    "        \n",
    "        # figure out the time each models cover\n",
    "        df[['start', 'stop']] = df['filename'].apply(extractDates).apply(pd.Series)\n",
    "        \n",
    "        df.to_csv(f'/home/users/chingosa/TropExt/CMIP6_Analysis/TempData/Paths_{model}.csv')   # Saves to CSV\n",
    "        return None\n",
    "# func.parallel_execution(findFilePaths, models, processes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de467628-70fb-4d6a-8c8d-02dc037cf216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c13c3-91f7-4590-8ab5-762374439d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bba8a-bbe6-49d0-871c-aaf48d3083a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451f7d2-ca96-46d3-b5f9-f83bc1ec49e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297801\n",
      "we tried to find information for 72\n",
      "We are unable to find any files for 0 models\n",
      "We have yet to process 2\n",
      "we can find 39 models that include ['tas', 'huss'] for 4x\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "notIn = []\n",
    "for model in models:\n",
    "    try:\n",
    "        df_add = pd.read_csv(f'TempData/Paths_{model}.csv', index_col=None)\n",
    "        df_add = df_add.loc[:, ~df_add.columns.str.contains('^Unnamed')]\n",
    "\n",
    "        \n",
    "        df = pd.concat([df, df_add], ignore_index=True)\n",
    "    except:\n",
    "        # print(f'{model} not included')\n",
    "        notIn.append(model)\n",
    "# df = df.dropna() # some opendap_urls are not found - so get rid of those\n",
    "df[['r', 'i', 'p', 'f']] = pd.DataFrame(\n",
    "    df['filename'].apply(extract_SimNums).tolist(),\n",
    "    index=df.index\n",
    ")\n",
    "        \n",
    "df['Varient'] = df.filename.apply(extract_varientID)\n",
    "df['period'] = df.filename.apply(getPeriod)\n",
    "df['Var'] = df.filename.apply(whichVar)\n",
    "df['grid'] = df.filename.apply(whichGrid)\n",
    "df['timeStep'] = df.filename.apply(whichTimestep)\n",
    "\n",
    "# figure out the time each models cover\n",
    "df[['start', 'stop']] = df['filename'].apply(extractDates).apply(pd.Series)\n",
    "df['experiment'] = df.filename.apply(getExperiment)\n",
    "\n",
    "dfAll = df\n",
    "print(len(dfAll))\n",
    "print(f'we tried to find information for {len(models)}')\n",
    "print(f'We are unable to find any files for {len(set(models) - set(df.model.unique())- set(notIn))} models')\n",
    "print(f'We have yet to process {len(notIn)}')\n",
    "\n",
    "insistVars = ['tas', 'huss']\n",
    "allVars = ['tas', 'huss', 'ps']\n",
    "\n",
    "dfNew = []\n",
    "for experiment in ['4x', 'SSP245']:\n",
    "    df = dfAll[dfAll['experiment'] == experiment]\n",
    "\n",
    "    # some pandas magic to find models where all the insisted upon vars are included\n",
    "    groups = df.groupby(['model',  'Var', 'period'], as_index=False).Var.value_counts()\n",
    "    \n",
    "    if len(insistVars) == 0:\n",
    "        tasHuss = groups.groupby(['model'], as_index = False).model.value_counts()\n",
    "    else:\n",
    "        tasHuss = groups[~groups['Var'].isin(list(set(allVars) - set(insistVars)))].groupby(['model'], as_index = False).model.value_counts()\n",
    "    \n",
    "    goodModels = tasHuss[tasHuss['count'] == len(insistVars)*2].reset_index(drop=True).model    # the 2 in here will mess up if you have a dif number of periods\n",
    "    \n",
    "    print(f'we can find {len(goodModels)} models that include {insistVars} for {experiment}')\n",
    "    \n",
    "    df = df[df['model'].isin(goodModels)]\n",
    "\n",
    "    shared_combinations = {}\n",
    "    \n",
    "    for (period, model), group in df.groupby(['period', 'model']):\n",
    "        combinations_per_variable = (\n",
    "            group.groupby('Var')[['r', 'f', 'p', 'i']]\n",
    "            .apply(lambda var_group: set(tuple(x) for x in var_group.to_numpy()))\n",
    "            .tolist()\n",
    "        )\n",
    "        if combinations_per_variable:\n",
    "            shared_combinations_for_group = set.intersection(*combinations_per_variable)\n",
    "        else:\n",
    "            shared_combinations_for_group = set()\n",
    "        shared_combinations[(period, model)] = shared_combinations_for_group\n",
    "    \n",
    "    filtered_dfs = []\n",
    "    \n",
    "    for (period, model), combinations in shared_combinations.items():\n",
    "        if combinations:  \n",
    "            first_combination = next(iter(combinations))  \n",
    "            \n",
    "            filtered_df = df[\n",
    "                (df['period'] == period) & \n",
    "                (df['model'] == model) & \n",
    "                (df[['r', 'f', 'p', 'i']].apply(tuple, axis=1) == first_combination)\n",
    "            ]\n",
    "            filtered_dfs.append(filtered_df)\n",
    "            \n",
    "    df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "    df = df[df['Var'] != 'ps']    # we are going to only use monthly ps to keep things consistant\n",
    "    for model in df.model.unique():\n",
    "        checkEmpty = False\n",
    "        for period in df.period.unique():\n",
    "            if len(df[(df['model'] == model) & (df['period'] == period)]) == 0: checkEmpty = True\n",
    "    \n",
    "        if checkEmpty:\n",
    "            df = df[~(df['model'] == model)]\n",
    "    dfNew += [df]\n",
    "\n",
    "dfNew = pd.concat(dfNew).reset_index(drop=True)\n",
    "\n",
    "# this line removes duplicate files with the same grid\n",
    "dfNew.groupby(['model', 'period', 'Var']).apply(lambda g: g[g['grid'] == g['grid'].unique()[0]]).reset_index(drop=True, inplace=True)\n",
    "dfAll.to_csv('TempData/Allpaths.csv', index = None)\n",
    "dfNew.to_csv('TempData/tasHusspaths.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81286bc8-825c-407a-a4c4-e3bfe80d25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Litness check for varient IDs\n",
    "# for model in dfNew.model.unique():\n",
    "#     bools = []\n",
    "#     print(model)\n",
    "#     for period in dfNew.period.unique():\n",
    "#         print(set(dfNew[(dfNew['model'] == model) & (dfNew['period'] == period)].Varient.unique()))\n",
    "\n",
    "# len(dfNew.model.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ffac0f4-9e7f-49d6-8e56-8434aa50a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of models where we need monthly pressure:\n",
    "\n",
    "# getPSmodel = []\n",
    "# getPSperiod = []\n",
    "\n",
    "# for model in dfNew.model.unique():\n",
    "#     for period in ['abrupt-4xCO2','piControl','historical','ssp245']:\n",
    "#         df = dfAll[((dfAll['period'] == period) & (dfAll['Var'] == 'ps'))&(dfAll['model'] == model)]\n",
    "#         if df.timeStep.unique() == ['day']:\n",
    "#             getPSmodel += [model]\n",
    "#             getPSperiod += [period]\n",
    "\n",
    "\n",
    "# getPSmodel\n",
    "# for i in np.arange(len(getPSmodel)):\n",
    "#     files = []\n",
    "#     model = getPSmodel[i]    \n",
    "#     period = getPSperiod[i]\n",
    "#     print(model, period)\n",
    "    \n",
    "#     query = conn.new_context(project=\"CMIP6\",     \n",
    "#                                          experiment_id=period,\n",
    "#                                          source_id = model,\n",
    "#                                          frequency = 'mon', \n",
    "#                                          variable_id = 'ps')\n",
    "\n",
    "       \n",
    "#     results = query.search()\n",
    "#     for i in range(len(results)):\n",
    "#         try:\n",
    "#             hit = results[i].file_context().search()\n",
    "#         except:\n",
    "#             hit = results[i].file_context().search()\n",
    "#         files += list(map(lambda f: {'model': model,\n",
    "#                                      'filename': f.filename, \n",
    "#                                      'download_url': f.download_url, \n",
    "#                                      'opendap_url': f.opendap_url}, hit))\n",
    "#     df = pd.DataFrame.from_dict(files)\n",
    "#     dfOld = pd.read_csv(f'TempData/Paths_{model}.csv')\n",
    "#     dfOld = dfOld[~((dfOld['period'] == period) & (dfOld['Var'] == 'ps'))&(dfOld['model'] == model)] # removes the daily ps\n",
    "#     df = pd.concat([df, dfOld], ignore_index=True)\n",
    "#     df.to_csv(f'TempData/Paths_{model}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "755014c3-a2bb-4c80-8c0b-2d19d9d090ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "719f2971-0b30-44e4-9bcf-e73994c6b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "# notIn = []\n",
    "# for model in models:\n",
    "#     try:\n",
    "#         df_add = pd.read_csv(f'TempData/Paths_{model}.csv', index_col=0)\n",
    "#         df = pd.concat([df, df_add], ignore_index=True)\n",
    "#     except:\n",
    "#         # print(f'{model} not included')\n",
    "#         notIn.append(model)\n",
    "# # df = df.dropna() # some opendap_urls are not found - so get rid of those\n",
    "# df[['r', 'i', 'p', 'f']] = pd.DataFrame(\n",
    "#     df['filename'].apply(extract_SimNums).tolist(),\n",
    "#     index=df.index\n",
    "# )\n",
    "        \n",
    "# df['Varient'] = df.filename.apply(extract_varientID)\n",
    "# df['period'] = df.filename.apply(getPeriod)\n",
    "# df['Var'] = df.filename.apply(whichVar)\n",
    "# df['grid'] = df.filename.apply(whichGrid)\n",
    "# df['timeStep'] = df.filename.apply(whichTimestep)\n",
    "\n",
    "# # figure out the time each models cover\n",
    "# df[['start', 'stop']] = df['filename'].apply(extractDates).apply(pd.Series)\n",
    "# df['experiment'] = df.filename.apply(getExperiment)\n",
    "# df = df[~df['filename'].str.contains('Emon', na=False)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69b3b889-2a67-40b7-8d5f-2cea84d7af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# varsIn = df.groupby(['model', 'experiment', 'period'], as_index=False).Var.unique()\n",
    "# varsIn = varsIn[varsIn['Var'].apply(lambda x: set(x) == set(['tas', 'huss', 'ps']))].reset_index(drop=True)\n",
    "\n",
    "# experimentBoth = varsIn.groupby(['model', 'experiment'], as_index=False).period.nunique()\n",
    "# experimentBoth[experimentBoth.period == 2].reset_index(drop=True)[['model', 'experiment']]\n",
    "\n",
    "# filtered_experiment = experimentBoth[experimentBoth.period == 2].reset_index(drop=True)[['model', 'experiment']]\n",
    "\n",
    "# # Step 2: Merge with the original DataFrame to filter matching rows\n",
    "# filtered_df = df.merge(filtered_experiment, on=['model', 'experiment'], how='inner')\n",
    "# filtered_df.groupby(['model', 'experiment'], as_index=False).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b400bc3-250c-4310-b950-76806890d5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e2304-2cc1-43d4-9411-317323c50ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This section adds the ps data back in with the rest of it bc I'm an idiot\n",
    "\n",
    "dfNew = pd.read_csv('TempData/tasHusspaths.csv', index_col = None)\n",
    "dfAll = pd.read_csv('TempData/Allpaths.csv', index_col = None)\n",
    "dfPS = dfAll[dfAll['Var'] == 'ps'].reset_index(drop=True)\n",
    "dfPS = dfPS[~dfPS['filename'].str.contains('Emon|AERmon', na=False)].reset_index(drop=True)\n",
    "psDF_list = []\n",
    "# dfNew.merge(dfPS, on=['experiment', 'period', 'model', 'Varient', 'grid'], how='inner')\n",
    "for experiment in dfNew.experiment.unique():\n",
    "    expDF = dfNew[dfNew['experiment'] == experiment]\n",
    "    for model in expDF.model.unique():\n",
    "        for period in expDF.period.unique():\n",
    "            modelDF = expDF[(expDF['model'] == model) & (expDF['period'] == period)].drop_duplicates(subset='filename', keep='first')\n",
    "            Varient = modelDF.Varient.unique()\n",
    "                \n",
    "            modelPSDF = dfPS[((dfPS['model'] == model) & (dfPS['period'] == period))&(dfPS['Varient'] == Varient[0])].drop_duplicates(subset='filename', keep='first')\n",
    "            modelPSDF['monType'] = modelPSDF.filename.apply(extract_monType)\n",
    "            if len(modelPSDF['monType'].unique()) >1:\n",
    "                modelPSDF = modelPSDF[modelPSDF['monType'] == modelPSDF['monType'].unique()[0]]\n",
    "            modelPSDF = modelPSDF.drop(columns = ['monType'])\n",
    "            psDF_list += [modelPSDF]\n",
    "psDF_list = pd.concat(psDF_list, ignore_index=True)\n",
    "df = pd.concat([dfNew, psDF_list], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e7d18-55e7-4e25-9768-62014fff2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each variable - what is the minimum and maximum times they cover\n",
    "stopMax = df.groupby(['model', 'period', 'Var'], as_index=False).stop.max()\n",
    "startMin = df.groupby(['model', 'period', 'Var'], as_index=False).start.min()\n",
    "\n",
    "# find the overlaping times shared for each model and period\n",
    "stopShared = stopMax.groupby(['model', 'period'], as_index=False).min()\n",
    "startShared = startMin.groupby(['model', 'period'], as_index=False).max()\n",
    "\n",
    "span_df = pd.DataFrame({'model': startShared.model, \n",
    "              'period': startShared.period, \n",
    "              'span': stopShared.stop - startShared.start,\n",
    "              'startShared': startShared.start,\n",
    "              'stopShared': stopShared.stop})\n",
    "\n",
    "# find models that don't have shared data that spans more than 20 years\n",
    "insufficientSpan = []\n",
    "for model in span_df.model.unique():\n",
    "    spans = span_df[span_df['model'] == model].span.to_numpy()\n",
    "    if any(v < 20 for v in spans):\n",
    "        insufficientSpan.append(model)\n",
    "\n",
    "print('the following models dont cover a shared 20 years: ', insufficientSpan)\n",
    "\n",
    "# remove models with insufficient span\n",
    "df = df[~df.model.isin(insufficientSpan)]\n",
    "\n",
    "def inspan(row, padstart, padend):\n",
    "    return max(row['start'], padstart) <= min(row['stop'], padend)\n",
    "\n",
    "dfs = []  # Store results for all models and periods\n",
    "i = 0 \n",
    "for model in df.model.unique():\n",
    "    for period in df.period.unique():\n",
    "        i+=1\n",
    "        # Calculate padstart and padend based on `span_df`\n",
    "        mask = (span_df.model == model) & (span_df.period == period)\n",
    "        if mask.sum() > 0:  # Ensure there's a match in span_df\n",
    "            end = int(span_df.loc[mask, 'stopShared'].iloc[0])  # Extract the scalar value safely\n",
    "            padend = end + 3\n",
    "            padstart = end - 20 - 3\n",
    "            \n",
    "            filtered_df = df[(df.model == model) & (df.period == period)].copy()\n",
    "\n",
    "            # Apply inspan to the filtered DataFrame\n",
    "            filtered_df.loc[:, 'inspan'] = filtered_df.apply(\n",
    "                lambda row: inspan(row, padstart, padend), axis=1\n",
    "            )\n",
    "            \n",
    "            dfs.append(filtered_df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(len(df.model.unique()))\n",
    "df = df[df.inspan].reset_index(drop=True)\n",
    "print(len(df.model.unique()))\n",
    "span_df = span_df[~span_df.model.isin(insufficientSpan)].reset_index(drop = True)\n",
    "\n",
    "df = df.drop_duplicates(subset='filename', keep='first')\n",
    "\n",
    "## A little routine for removing files with multiple grid options\n",
    "for model in df.model.unique():\n",
    "    for period in df.period.unique():\n",
    "        for Var in df.Var.unique():\n",
    "            q = df[((df['model'] == model) & (df['period'] == period))&(df['Var'] == Var)].reset_index(drop = True)\n",
    "            if len(q.grid.unique()) >1:\n",
    "                q = q[q['grid'] == q.grid.unique()[0]]\n",
    "            dfAdd += [q]\n",
    "\n",
    "df = pd.concat(dfAdd, ignore_index=True)\n",
    "df.to_csv('TempData/4xCO2_SSP245_reducedFilePaths.csv', index = None)\n",
    "# span_df.to_csv('span_df.csv')\n",
    "# df.to_csv('CO2_4x_url_reduced_wMon.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4c641-3fa9-4277-9d92-6b14750f54de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f65c2cd-b1b1-4b25-a127-866dc2edf70b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef146e-3f3f-4cd5-ac26-16389d879a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
